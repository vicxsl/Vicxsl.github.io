---
title: 大数据面试题100道
date: 2021-08-25 12:50:07
permalink: /pages/5ff074/
categories:
  - 更多
  - 面试
tags:
  - 
---

# 大数据工程师面试题

## 选择题

1. 下面哪个程序负责HDFS数据存储。

   a) NameNode 	b) Jobtracker 	c) Datanode 	d) secondaryNameNode 	e) tasktracker 

   答案 C datanode

   

2. HDfS 中的 block 默认保存几份？ 

   a) 3 份 	b) 2 份 	c) 1 份 	d) 不确定 

   答案 A 默认 3 份

   

3. 下列哪个程序通常与 NameNode 在一个节点启动? 

   a) SecondaryNameNode 	b) DataNode 	c) TaskTracker 	d) Jobtracker 

   答案 D

   

4. HDFS 默认 Block Size 

   a) 32MB	b) 64MB	c) 128MB 

   答案：B

   

5. 下列哪项通常是集群的最主要瓶颈 

   a) CPU 	b) 网络 	c) 磁盘 IO 	d) 内存 

   答案：C 磁盘

> 首先集群的目的是为了节省成本，用廉价的 pc 机，取代小型机及大型机。小型机和大型机 有什么特点？ 
>
> 1.cpu 处理能力强 
>
> 2.内存够大，所以集群的瓶颈不可能是 a 和 d 
>
> 3.如果是互联网有瓶颈，可以让集群搭建内网。每次写入数据都要通过网络（集群是内网）， 然后还要写入 3 份数据，所以 IO 就会打折扣。



6. 关于 SecondaryNameNode 哪项是正确的？

   a) 它是 NameNode 的热备 	b) 它对内存没有要求 	c) 它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间 	d) SecondaryNameNode 应与 NameNode 部署到一个节点 

   答案 C

   

7. 下列哪项可以作为集群的管理？ 

   a) Puppet 	b) Pdsh 	c) Cloudera Manager d) Zookeeper 

   答案 ABD 	具体可查看什么是 Zookeeper，Zookeeper 的作用是什么，在 Hadoop 及 hbase 中具体作 用是什么。

   

8. Client 端上传文件的时候下列哪项正确 

   a) 数据经过 NameNode 传递给 DataNode 	b) Client 端将文件切分为 Block，依次上传 	c) Client 只上传数据到一台 DataNode，然后由 NameNode 负责 Block 复制工作 

   答案 B 分析：Client 向 NameNode 发起文件写入的请求。NameNode 根据文件大小和文件块配置 情况，返回给 Client 它所管理部分 DataNode 的信息。Client 将文件划分为多个 Block，根据 DataNode 的地址信息，按顺序写入到每一个 DataNode 块中。具体查看 HDFS 体系结构简介及优缺点。



9. 下列哪个是 Hadoop 运行的模式 

   a) 单机版 	b) 伪分布式 	c) 分布式 

   答案 ABC 单机版,伪分布式只是学习用的。

---

## 面试题

### Hadoop 的核心配置是什么？ 

Hadoop 的核心配置通过两个 xml 文件来完成：1，hadoop-default.xml；2，hadoop-site.xml。 这些文件都使用 xml 格式，因此每个 xml 中都有一些属性，包括名称和值，但是当下这些文 件都已不复存在。



### 那当下又该如何配置？

Hadoop 现在拥有 3 个配置文件：1，core-site.xml；2，hdfs-site.xml；3，mapred-site.xml。这 些文件都保存在 conf/子目录下。



### “jps”命令的用处？

 这个命令可以检查 Namenode、Datanode、Task Tracker、 Job Tracker 是否正常工作。



### ❤️mapreduce 的原理?

![image-20210825130550811](https://gitee.com/vicxsl/img/raw/master/img/1629867951423/image-20210825130550811.png)

1. 单行文件内容作为输入
2. 分布式的DataNode处理数据
   1. 通过Mapper处理文件，处理偏移量和数据分割
   2. content 循环输出分割好的内容
3. 拷贝数据到其他DataNode（分区好之后？）
4. Reducer处理，加工后输出数据

### HDFS 存储的机制?

### HDFS 写流程

![image-20210825132443536](https://gitee.com/vicxsl/img/raw/master/img/1629869083912/image-20210825132443536.png)

流程： 

1.  client 链接 namenode 存数据 

2. namenode 记录一条数据位置信息（元数据），告诉 client 存哪。 

3. client 用 hdfs 的 api 将数据块（默认是 64M）存储到 datanode 上。 

4.  datanode 将数据水平备份。并且备份完将反馈 client。 

5. client 通知 namenode 存储块完毕。 

6. namenode 将元数据同步到内存中。 

7. 另一块循环上面的过程。

   

### HDFS读流程

![image-20210825132702811](https://gitee.com/vicxsl/img/raw/master/img/1629869223178/image-20210825132702811.png)

流程：

1. client 链接 namenode，查看元数据，找到数据的存储位置。 2. 
2. client 通过 hdfs 的 api 并发读取数据。 
3. 关闭连接。



### 举一个简单的例子说明 mapreduce 是怎么来运行的 ?

wordcount 的例子



### 用 mapreduce 来实现下面需求？ 

现在有 10 个文件夹,每个文件夹都有 1000000 个 url.现在让你找出 top1000000url。 

解答：topk



(还可以用 treeMap, 到 1000000 了每来一个都加进去, 删掉最小的)



### hadoop 中 Combiner 的作用?

combiner 是 reduce 的实现，在 map 端运行计算任务，减少 map 端的输出数据。 作用就是优化。 

但是 combiner 的使用场景是 mapreduce 的 map 和 reduce 输入输出一样。



### 简述 hadoop 安装

![image-20210825133000964](https://gitee.com/vicxsl/img/raw/master/img/1629869401349/image-20210825133000964.png)

1. 创建 hadoop 帐户.
2. setup.改IP.
3. 安装java，并修改/etc/profile 文件，配置 java 的环境变量.
4. 修改 Host 文件域名.
5. 安装 SSH，配置无密钥通信.
6. 解压hadoop.
7. 配置 conf 文件下 hadoop-env.sh、core-site.sh、 mapre-site.sh、hdfs-site.sh.
8. 配置hadoop的环境变量,
9. Hadoop namenode -format
10. Start-all



### 请列出 hadoop 进程名

![image-20210825133236037](https://gitee.com/vicxsl/img/raw/master/img/1629869556412/image-20210825133236037.png)

1. namenode : 管理集群，并记录 datanode 文件信息.
2. Secondname:可以做冷备，对一定范围内数据做快照性备份.
3. Datanode:存储数据
4. Jobtracker :管理任务，并将任务分配给 tasktracker.
5. Tasktracker:任务执行方.



### 解决下面的错误

![image-20210825133409119](https://gitee.com/vicxsl/img/raw/master/img/1629869649566/image-20210825133409119.png)

1、 权限问题，可能曾经用 root启动过集群。 ( 例如 hadoop搭建的集群 ,是tmp/hadoop hadoop/..... )
2、 可能是文件夹不存在( Directory does not exist )
3、 解决 : 删掉 tmp下的那个文件 ,或改成当前用户（ Directory /tmp/hadoop-root/ ）



### 写出下面的命令

![image-20210825133650988](https://gitee.com/vicxsl/img/raw/master/img/1629869811385/image-20210825133650988.png)

1. hadoop job -list 

2. 拿到job-id  shadoop job -kill job-id

3. Hadoop fs -rmr /tmp/aaa

4. 加新节点时:
   Hadoop-daemon.sh start datanode

   Hadoop-daemon.sh start tasktracker

5. 删除时:

   Hadoop mradmin -refreshnodes
   Hadoop dfsadmin -refreshnodes



### 简述 hadoop的调度器

![image-20210826131756492](https://gitee.com/vicxsl/img/raw/master/img/1629955077154/image-20210826131756492.png)

Fifo schedular : 默认，先进先出的原则。
Capacity schedular : 计算能力调度器，选择占用最小、优先级高的先执行，依此类推。
Fair schedular: 公平调度，所有的 job 具有相同的资源。



### 列出你开发 mapreduce的语言

![image-20210826131927559](https://gitee.com/vicxsl/img/raw/master/img/1629955167927/image-20210826131927559.png)

java



### 书写程序

![image-20210826132034992](https://gitee.com/vicxsl/img/raw/master/img/1629955235358/image-20210826132034992.png)

wordcount



### 不同语言的优缺点

![image-20210826132057466](https://gitee.com/vicxsl/img/raw/master/img/1629955257822/image-20210826132057466.png)

hadoop

hadoop是 java写的， java的集成效果最好，并且平台环境统一。



### hive有哪些保存元数据的方式，个有什么特点。

![image-20210826132128077](https://gitee.com/vicxsl/img/raw/master/img/1629955288440/image-20210826132128077.png)

1、 内存数据库 derby，安装小，但是数据存在内存，不稳定
2、 mysql数据库，数据存储模式可以自己设置，持久化好，查看方便。



combiner和 partition的作用

![image-20210826132158158](https://gitee.com/vicxsl/img/raw/master/img/1629955318528/image-20210826132158158.png)

combiner是 reduce的实现，在 map端运行计算任务，减少 map端的输出数据。
**作用就是优化。**
但是combiner的使用场景是mapreduce的 map输出结果和 reduce输入输出一样。



**partition的默认实现是 hashpartition，是 map端将数据按照 reduce个数取余**，进行分区，
不同的 reduce来 copy自己的数据。
partition的作用是将数据分到不同的 reduce进行计算，加快计算效果。



### hive内部表和外部表的区别

内部表：加载数据到 hive所在的 hdfs目录， 删除时，元数据和数据文件都删除。
外部表：不加载数据到 hive所在的 hdfs目录，删除时，只删除表结构。



### hbase的 rowkey怎么创建好？列族怎么创建比较好？

hbase存储时，数据按照 Row key的字典序 (byte order)排序存储。设计 key时，要充分排序
存储这个特性，将经常一起读取的行存储放到一起。 (位置相关性 )

一个列族在数据底层是一个文件，所以将经常一起查询的列放到一个列族中，列族尽量少，减少文件的寻址时间。减少文件的寻址时间。



### ❤️用 mapreduce怎么处理数据倾斜问题？

数据倾斜：

map /reduce程序执行时， reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个 key的条数比其他key多很多（有时是百倍或者千倍之多），这条 key所在的 reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完，此称之为数据倾斜。
用hadoop程序进行数据关联时，常碰到数据倾斜的情况，这里提供一种解决方法。

自己实现**partition类，用** **key和 value相加取 hash值**：

方式1：
源代码：

```java
public int getPartition(K key, V value,int numReduceTasks) 
{ 
    return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
}

修改后
public int getPartition(K key, V value,int numReduceTasks) 
{ 
    return ((（key).hashCode()+value.hashCode())& Integer.MAX_VALUE) % numReduceTasks;
}
```



方式2：

```java
public class HashPartitioner<K, V> extends Partitioner<K, V> {
private int aa= 0;
    
/** Use {@link Object#hashCode()} to partition. */
public int getPartition(K key, V value,int numReduceTasks) 
{
return (key.hashCode()+(aa++) & Integer.MAX_VALUE) % numReduceTasks;
}
```

> 重新分区器



### hadoop框架中怎么来优化

1. 从应用程序角度进行优化。由于 mapreduce是迭代逐行解析数据文件的，怎样在迭代的情况下，编写高效率的应用程序，是一种优化思路。
2.  对 Hadoop参数进行调优。当前 hadoop系统有 190多个配置参数，怎样调整这些参数，使 hadoop作业运行尽可能的快，也是一种优化思路。
3. 从系统实现角度进行优化。这种优化难度是最大的，它是从 hadoop实现机制角度，发现当前 Hadoop设计和实现上的缺点，然后进行源码级地修改。该方法虽难度大，但往往效果明显。
4. linux内核参数调整



### 从应用程序角度进行优化

1. 避免不必要的 reduce任务

   如果mapreduce程序中 reduce是不必要的，那么我们可以在 map中处理数据 , Reducer设置
   为 0。这样 避免了多余的 reduce任务。

2. 为 job添加一个 Combiner

   为job添加一个 combiner可以大大减少 shuffle阶段从 map task拷贝给远程 reduce task的数据量。一般而言， combiner与 reducer相同。

3. 根据处理数据特征使用最适合和简洁的 Writable类型
   Text对象使用起来很方便，但它在由数值转换到文本或是由 UTF8字符串转换到文本时都是低效的，且会消耗大量的 CPU时间。当处理那些非文本的数据时，可以使用二进制的 Writable类型，如 IntWritable FloatWritable等。二进制 writable好处：避免文件转换的消耗；使map task中间结果占用更少的空间。

4. 重用 Writable类型
   很多MapReduce用户常犯的一个错误是，在一个 map/reduce方法中为每个输出都创建Writable对象。例如，你的 Wordcout mapper方法可能这样写：

   ```java
   public void map(...) {
   …
   for (String word : words) {
   	output.collect(new Text(word), new IntWritable(1));
   	}
   }
   ```

   这样会导致程序分配出成千上万个短周期的对象。Java垃圾收集器就要为此做很多的工作。垃圾收集器就要为此做很多的工作。更有效的写法是：更有效的写法是：

   抽取对象

   ```java
   class MyMapper … {
   Text wordText = new Text();
       
   IntWritable one = new IntWritable(1);
       
   public void map(...) {
       for (String word: words) {
               wordText.set(word);
               output.collect(wordText, one);
           }
   	}
   }
   ```

   

5.  使用使用StringBuffer而不是而不是String

   当需要对字符串进行操作时，**使用StringBuffer而不是而不是String，String是是read--only的，如果对的，如果对它进行修改，会产生临时对象**，而StringBuffer是可修改的，不会产生临时对象。是可修改的，不会产生临时对象。
   

### 对参数进行调优

查看linux的服务，可以关闭不必要的服务

 ntsysv 



**停止打印服务**

/etc/init.d/cups stop

chkconfig cups off



**关闭ipv6** 

vim /etc/modprobe.conf 

添加内容 

alias net-pf-10 off alias ipv6 off



**调整文件最大打开数** 

查看： ulimit -a  结果：open files (-n) 1024 

临时修改： ulimit -n 4096 

持久修改：

 vi /etc/security/limits.conf在文件最后加上：

* soft nofile 65535
* hard nofile 65535 
* soft nproc 65535 
* hard nproc 65535



**修改linux内核参数**

vi /etc/sysctl.conf 

添加 

net.core.somaxconn = 32768 

web应用中listen函数的backlog默认会给我们内核参数的net.core.somaxconn限制到128，而nginx定义的NGX_LISTEN_BACKLOG默认为511，所以有必要调整这个值。 

调整swap分区什么时候使用：

查看：cat /proc/sys/vm/swappiness 

设置：vi /etc/sysctl.conf 

​	在这个文档的最后加上这样一行: vm.swappiness=10 

​	表示物理内存使用到90%（100-10=90）的时候才使用swap交换区



**关闭noatime**

vi /etc/fstab /dev/sda2/data ext3	 noatime,nodiratime 0 0



**设置readahead buffer** 

blockdev --setra READAHEAD 512 /dev/sda



**以下是修改mapred-site.xml文件**

**修改最大槽位数** 

槽位数是在各个tasktracker上的mapred-site.xml上设置的，默认都是2

```properties
<property> 
<name>mapred.tasktracker.map.tasks.maximum</name> 
#++++maptask的最大数
<value>2</value>
</property> 

<property> 
<name>mapred.tasktracker.reduce.tasks.maximum</name> 
#++++reducetask的最大数 
<value>2</value> 
</property>
```



**调整心跳间隔**

集群规模小于300时，心跳间隔为300毫秒 

```properties
mapreduce.jobtracker.heartbeat.interval.min 心跳时间 

mapred.heartbeats.in.second 集群每增加多少节点，时间增加下面的值 

mapreduce.jobtracker.heartbeat.scaling.factor 集群每增加上面的个数，心跳增多少
```



**启动带外心跳** 

```properties
mapreduce.tasktracker.outofband.heartbeat 默认是false
```



**配置多块磁盘** 

```properties
mapreduce.local.dir 
```



**配置RPC hander数目** 

```properties
mapred.job.tracker.handler.count 默认是10，可以改成50，根据机器的能力
```



**配置HTTP线程数目**

```properties
tasktracker.http.threads 默认是40，可以改成100 根据机器的能力
```



**选择合适的压缩方式**

```xml
以snappy为例： 
<property>
	<name>mapred.compress.map.output</name>
	<value>true</value> 
</property> 

<property>
	<name>mapred.map.output.compression.codec</name>
	<value>org.apache.hadoop.io.compress.SnappyCodec</value>
</property>
```



**启用推测执行机制**

> ​		推测执行(Speculative Execution)是指在分布式集群环境下，因为程序BUG，负载不均衡或者资源分布不均等原因，造成同一个job的多个task运行速度不一致，有的task运行速度明显慢于其他task（比如：一个job的某个task进度只有10%，而其他所有task已经运行完毕），则这些task拖慢了作业的整体执行进度，为了避免这种情况发生，Hadoop会为该task启动备份任务，让该speculative task与原始task同时处理一份数据，哪个先运行完，则将谁的结果作为最终结果。 
>
> ​		推测执行优化机制采用了典型的以空间换时间的优化策略，它同时启动多个相同task（备份任务）处理相同的数据块，哪个完成的早，则采用哪个task的结果，这样可防止拖后腿Task任务出现，进而提高作业计算速度，但是，这样却会占用更多的资源，在集群资源紧缺的情况下，设计合理的推测执行机制可在多用少量资源情况下，减少大作业的计算时间。

```properties
mapred.map.tasks.speculative.execution 默认是true

mapred.rduce.tasks.speculative.execution 默认是true
```



**设置失败容忍度**

```properties
mapred.max.map.failures.percent 作业允许失败的map最大比例 默认值0，即0%

mapred.max.reduce.failures.percent 作业允许失败的reduce最大比例 默认值0，即0%

mapred.map.max.attemps 失败后最多重新尝试的次数 默认是4 

mapred.reduce.max.attemps 失败后最多重新尝试的次数 默认是4
```



**启动jvm重用功能**

```properties
mapred.job.reuse.jvm.num.tasks 默认值1，表示只能启动一个task，若为-1，表示可以最多运行数不限制
```



**设置任务超时时间**

```properties
mapred.task.timeout 默认值600000毫秒，也就是10分钟。
```



**合理的控制reduce的启动时间**

```properties
mapred.reduce.slowstart.completed.maps 默认值0.05 表示map任务完成5%时，开始启动reduce任务
```



**跳过坏记录** 

当任务失败次数达到该值时，才会进入skip mode，即启用跳过坏记录数功能,也就是先试几次，不行就跳过

mapred.skip.attempts.to.start.skipping 默认值 2 



```properties
map最多允许跳过的记录数 

mapred.skip.map.max.skip.records 默认值0，为不启用 

reduce最多允许跳过的记录数 
mapred.skip.reduce.max.skip.records 默认值0，为不启用

换记录存放的目录
mapred.skip.out.dir 默认值${mapred.output.dir}/_logs/
```



### 我们开发 job 时，是否可以去掉 reduce 阶段

可以。设置 reduce 数为 0 即可。



### datanode 在什么情况下不会备份 

datanode 在强制关闭或者非正常断电不会备份。



### combiner 出现在那个过程

出现在 map 阶段的 map 方法后。



### 😺hdfs 的体系结构 

hdfs 有 namenode、secondraynamenode、datanode 组成。 

为 n+1 模式 

namenode 负责管理 datanode 和记录元数据 

secondraynamenode 负责合并日志 

datanode 负责存储数据



### 3 个 datanode 中有一个 datanode 出现错误会怎样？ 

这个 datanode 的数据会在其他的 datanode 上重新做备份。



### 描述一下 hadoop 中，有哪些地方使用了缓存机制， 作用分别是什么？

在 mapreduce 提交 job 的获取 id 之后，会将所有文件存储到分布式缓存上，这样文件可以 被所有的 mapreduce 共享。



### 如何确定 hadoop 集群的健康状态 

通过页面监控,脚本监控。



### 生产环境中为什么建议使用外部表？ 

1、因为外部表不会加载数据到 hive，减少数据传输、数据还能共享。 

2、hive 不会修改数据，所以无需担心数据的损坏 

3、 删除表时，只删除表结构、不删除数据。



## 面试真题

### 请选择你熟练掌握的hadoop版本，并基于此回答下列问题

hadoop1.0	

#### hadoop的核心配置文件名称是什么？

core-site.xml

#### "jps"命令的用处？

查看hadoop节点进程

#### 如何检查namenode是否正常运行？重启namenode的命令是什么？

通过节点信息和浏览器查看，通过脚本监控
hadoop deamon.sh start namenode
hdfs deamon.sh start namenode

#### 避免namenode故障导致集群宕机的解决方法是什么？

自己书写脚本监控重启

#### hbase数据库对行键的设计要求是什么？

行健以字典序排列，设计时充分利用这个特点，将经常一起查询的行健设计在一起，例
如时间戳结尾，用户名开头（位置相关性）



### Hadoop面试业务题

**业务场景：**

#### 用户访问网站时，每个页面会上报一条pv数据，同时做一些业务操作，会上报事件数据，如：

1. 用户浏览页面（PV）
2. 用户事件行为（开户，下单买基金......）
3. 页面click点击（包含各种超链接，可点击按钮，radio，checkbox......）

每日，以上3项的上报数大致10000000



**业务需求：**

1. 需要按时间维度（天，周），某种业务维度（开户，买基金...），定时做统计（总人数，金额等）。

例：过去一周（隔日）的pv，uv数，交易总金额。



2. 需要回溯历史数据，如：过去某个时间点（段），访问过某页面的用户，在某个时间点（段）导致某种业务发生的统计数据。

例：在2015-01-01至2015-01-31访问A页面，并在2015-01-01至2015-03-31开户，下单的用户数



**技术方案：**

请给出你的设计方案，比如使用哪些技术框架，该框架起到的作用等。

1. 用hive分析业务数据即可

2. 讲数据导入到hive中

   sql的设计思路：多表关联

   1. 找到所有在 2015 01 01到 2015 01 31时间内访问 A页面的用户
   2. 在这些用户中筛选在 2015 01 01到 2015 03 31下单的用户
   3. 统计总数



### 你们数据库怎么导入 hive 的 ,有没有出现问题

在导入hive的时候，如果数据库中有 blob或者 text字段，会报错，解决方案在 sqoop笔记中。



### 公司技术选型可能利用 storm 进行实时计算 ,讲解一下storm

描述下storm的设计模式，是基于 work、 excutor、 task的方式运行代码，由 spout、 bolt组
成等等



### 一个 datanode 宕机 ,怎么一个流程恢复

将datanode数据删除，重新当成新节点加入即可。



### Hbase 的特性 ,以及你怎么去设计 rowkey 和 columnFamily ,怎么去建一个 table

hbase是列式数据库， rowkey是字典序的，设计时的规则同上。
每个列族是一个文件，将经常一起查询的列放到同一个列族中，减少文件的寻址时间。



### ❤️Redis,传统数据库 ,hbase,hive 每个之间的区别

- redis：分布式缓存，强调缓存，内存中数据
- 传统数据库：注重关系
- hbase：列式数据库，无法做关系数据库的主外键，用于存储海量数据，底层基于 hdfs
- hive：数据仓库工具，底层是 mapreduce。不是数据库，不能用来做用户的交互存储



### ❤️shuffle 阶段 ,你怎么理解的

shuffle的过程说清楚，目的说清楚



### Mapreduce 的 map 数量 和 reduce 数量 怎么确定 ,怎么配置

map的数量有数据块决定， reduce数量随便配置。



### storm实时计算

唯一难住我的是他说实时计算 ,storm 如果碰上了复杂逻辑 ,需要算很长的时间 ,你怎么去优化 ,怎么保证实时性



### Hive 你们用的是外部表还是内部表 ,有没有写过UDF,hive 的版本

外部表和内部表的区别



### Hadoop 的版本

1.04、 1.20都为稳定版，是两个常用的 hadoop1版本。



### 实时流式计算的结果内容有哪些 ,你们需要统计出来么



### 设计日志收集分析系统

日志分布在各个业务系统中。我们需要对当天的日志进行实时汇总统计，同时又能查询历史的汇总数据（可以围绕PV、UV、IP等指标进行阐述）

1. 通过 flume将不同系统的日志收集到 kafka中
2. 通过 storm实时的处理 PV、 UV、 IP
3. 通过 kafka的 consumer将日志生产到 hbase中。
4. 通过离线的 mapreduce或者 hive，处理 hbase中的数据




### 如果你来做技术分享，你会选择什么主题，课程安排时怎么样的？

大体分为3个部分 :
1、 离线 hadoop技术分享（ mapreduce、 hive）
2、 nosql数据库hbase分享
3、 实时流计算分享



### Hive语句实现WordCount

假设数据存储在hadoop下，路径为：/home/hadoop/wordcount里面全是一些单词

1. 建表
2. 分组（group by）统计wordcount

```sql
select word,count(1) from table1 group by word;
```



### 大数据相同字符比对

给定a，b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，找出a、b文件共同的url？

可以估计每个文件的大小为50G*64=298G，远远大于内存限制的4G.所以不可能将其完全加载到内存中处理。考虑采取**分而治之**的方法。

1. 将文件存储到 hdfs中，这样每个文件为 64M或者是 128M
2. 分别对两个文件的 url进行去重、排序输出，这样能排除a文件中相同的url，b文件也一样
3. 对 a、b两个文件处理后的结果进行 wordcount，并且在 reduce中判断单词个数，个数为 2的时候输出，这样就找到了 a、 b文件中的相同 url。
4. 此计算步骤中的每一步加载到内存中的文件大小都不会超过 64M，远远小于 4G。



### 😶一亿个数据获取前100个最大值（步骤及算法复杂度）

topk，强调使用treemap是为了节省计算空间



### 实时数据统计会用到哪些技术，他们各自的应用场景及区别是什么？

flume：日志收集系统，主要用于系统日志的收集
kafka：消息队列，进行消息的缓存和系统的解耦
storm：实时计算框架，进行流式的计算



### String和StringBuffer的区别，StringBuffer和StringBuilder的区别

简单地说，就是一个变量和常量的关系。StringBuffer对象的内容可以修改；而 String对象一旦产生后就不可以被修改，重新赋值其实是两个对象。

StringBuilder：线程非安全的
StringBuffer：线程安全的


当我们在字符串缓冲区被多个线程使用时， JVM不能保证 StringBuilder的操作是安全的，虽然他的速度最快。但是可以保证 StringBuffer是可以正确操作的。

当然大多数情况下就是我们是在单线程下进行的操作，所以大多数情况下是建议用 StringBuilder而不用StringBuffer的，就是速度的原因。



## 

### ❤️HashTable和HashMap、ArrayList和Vector、ArrayList和LinkedList的区别

#### HashMap不是线程安全的

hastmap是一个接口是map接口的子接口，是将键映射到值的对象，其中键和值都是对象，并且不能包含重复键，但可以包含重复值。HashMap允许null key和null value，而hashtable不允许。

HashTable是线程安全的一个Collection

HashMap是Hashtable的轻量级实现（非线程安全的实现），他们都完成了Map接口，主要区别在于HashMap允许空（null）键值（key）,由于非线程安全，效率上可能高于Hashtable。 HashMap允许将null作为一个entry的key或者value，而Hashtable不允许。 

HashMap把Hashtable的contains方法去掉了，改成containsvalue和containsKey。因为contains方法容易让人引起误解。 

Hashtable继承自Dictionary类，而HashMap是Java1.2引进的Map interface的一个实现。 最大的不同是，Hashtable的方法是Synchronize的，而HashMap不是，在多个线程访问Hashtable时，不需要自己为它的方法实现同步，而HashMap 就必须为之提供外同步。 Hashtable和HashMap采用的hash/rehash算法都大概一样，所以性能不会有很大的差。

#### HashTable对比HashMap

| hashmap   | 线程不安全 | 允许有null的键和值   | 效率高一点 | 方法不是Synchronize的要提供外同步 | 有containsvalue和containsKey方法 |
| --------- | ---------- | -------------------- | ---------- | --------------------------------- | -------------------------------- |
| hashtable | 线程安全   | 不允许有null的键和值 | 效率稍低   | 方法是是Synchronize的             | 有contains方法                   |

#### Hashtable和HashMap类有三个重要的不同之处。

第一个不同主要是历史原因。Hashtable是基于陈旧的Dictionary类的，HashMap是Java 1.2引进的Map接口的一个实现。

也许最重要的不同是Hashtable的方法是同步的，而HashMap的方法不是。这就意味着，虽然你可以不用采取任何特殊的行为就可以在一个多线程的应用程序中用一个Hashtable，但你必须同样地为一个HashMap提供外同步。一个方便的方法就是利用Collections类的静态的synchronizedMap()方法，它创建一个线程安全的Map对象，并把它作为一个封装的对象来返回。这个对象的方法可以让你同步访问潜在的HashMap。这么做的结果就是当你不需要同步时，你不能切断Hashtable中的同步（比如在一个单线程的应用程序中），而且同步增加了很多处理费用。

第三点不同是，只有HashMap可以让你将空值作为一个表的条目的key或value。HashMap中只有一条记录可以是一个空的key，但任意数量的条目可以是空的value。这就是说，如果在表中没有发现搜索键，或者如果发现了搜索键，但它是一个空的值，那么get()将返回null。如果有必要，用containKey()方法来区别这两种情况。

一些资料建议，当需要同步时，用Hashtable，反之用HashMap。但是，因为在需要时，HashMap可以被同步，HashMap的功能比Hashtable的功能更多，而且它不是基于一个陈旧的类的，所以有人认为，在各种情况下，HashMap都优先于Hashtable。

#### Vector & ArrayList

Vector的方法都是同步的 (Synchronized),是线程安全的 (thread safe)，而 ArrayList的方法不是，由于线程的同步必然要影响性能，因此 ,ArrayList的性能比Vector好。
2 当 Vector或 ArrayList中的元素超过它的初始大小时 ,Vector会将它的容量翻倍 ,而 ArrayList只增加50%的大小，这样 ,ArrayList就有利于节约内存空间。

#### linkedlist& ArrayList

ArrayList 采用的是数组形式来保存对象的，这种方式将对象放在连续的位置中，所以最大的缺点就是插入删除时非常麻烦

LinkedList 采用的将对象存放在独立的空间中，而且在每个空间中还保存下一个链接的索引 但是缺点就是查找非常麻烦 要丛第一个索引开始



### 多线程实现方式Thread和Runnable的区别？

#### 线程实现介绍

在java中可有两种方式实现多线程，一种是继承Thread类，一种是实现Runnable接口；

Thread类是在java.lang包中定义的。一个类只要继承了Thread类同时覆写了本类中的run()方法就可以实现多线程操作了，但是一个类只能继承一个父类，这是此方法的局限。

下面看例子：

```java
package org.thread.demo;
class MyThread extends Thread{
	private String name;
	
	public MyThread(String name) {
	super();
	this.name = name;
	}

	public void run(){
        for(int i=0;i<10;i++){
        System.out.println("线程开始："+this.name+",i="+i);
        }
	}
}

package org.thread.demo;
public class ThreadDemo01 {
	public static void main(String[] args) {
        MyThread mt1=new MyThread("线程a");
        MyThread mt2=new MyThread("线程b");
        mt1.run();
        mt2.run();
	}
}
```

但是，此时结果很有规律，先第一个对象执行，然后第二个对象执行，并没有相互运行。在JDK的文档中可以发现，**一旦调用start()方法，则会通过JVM找到run()方法**。下面启动start()方法启动线程：

```java
package org.thread.demo;
public class ThreadDemo01 {
	public static void main(String[] args) {
	MyThread mt1=new MyThread("线程a");
	MyThread mt2=new MyThread("线程b");
	mt1.start();
	mt2.start();
	}
};
```

这样程序可以正常完成交互式运行。那么为啥非要使用start();方法启动多线程呢？
在JDK的安装路径下，src.zip是全部的java源程序，通过此代码找到Thread中的start()方法的定义，可以发现此方法中使用了**private native void start0()**;

其中**native关键字**表示可以调用操作系统的底层函数，那么这样的技术成为JNI技术（java Native Interface）

#### Runnable接口

在实际开发中一个多线程的操作很少使用Thread类，而是通过Runnable接口完成。

```java
public interface Runnable{
	public void run();
}
```

例子：

```java
package org.runnable.demo;
class MyThread implements Runnable{
	private String name;
	public MyThread(String name) {
	this.name = name;
	}
    
	public void run(){
        for(int i=0;i<100;i++){
        System.out.println("线程开始："+this.name+",i="+i);
        }
	}
};
```



但是在使用Runnable定义的子类中没有start()方法，只有Thread类中才有。此时观察Thread类，有一个构造方法：public Thread(Runnable targer)此构造方法接受Runnable的子类实例，也就是说可以通过Thread类来启动Runnable实现的多线程。（start()可以协调系统的资源）：

```java
package org.runnable.demo;

import org.runnable.demo.MyThread;

public class ThreadDemo01 {
public static void main(String[] args) {
	MyThread mt1=new MyThread("线程a");
	MyThread mt2=new MyThread("线程b");
	new Thread(mt1).start();
	new Thread(mt2).start();
	}
}
```

两种实现方式的区别和联系：

在程序开发中只要是多线程肯定永远以实现Runnable接口为主，因为实现Runnable接口相比继承Thread类有如下好处：

> - 避免点继承的局限，一个类可以继承多个接口。
> - 适合于资源的共享

以卖票程序为例，通过Thread类完成：
```java
package org.demo.dff;

class MyThread extends Thread{

private int ticket=10;

    public void run(){
        for(int i=0;i<20;i++){
            if(this.ticket>0){
            System.out.println("卖票：ticket"+this.ticket--);
            }
    	}
	}
};
```

下面通过三个线程对象，同时卖票：

```java
package org.demo.dff;

public class ThreadTicket {
    public static void main(String[] args) {

        MyThread mt1=new MyThread();
        MyThread mt2=new MyThread();
        MyThread mt3=new MyThread();

        mt1.start();//每个线程都各卖了10张，共卖了30张票
        mt2.start();//但实际只有10张票，每个线程都卖自己的票
        mt3.start();//没有达到资源共享

    }
}
```

实际只有10张表，但是由于余票数量（ticket）没有变量共享，所以卖出了30张。

如果用Runnable就可以实现**资源共享**，下面看例子：

```java
    package org.demo.runnable;

    class MyThread implements Runnable {
        private int ticket = 10;

        public void run() {
            for (int i = 0; i < 20; i++) {
                if (this.ticket > 0) {
                    System.out.println("卖票：ticket" + this.ticket--);
                }
            }
        }
    };
     
    package org.demo.runnable;

    public class RunnableTicket {
        public static void main(String[] args) {
            MyThread mt = new MyThread();
            new Thread(mt).start();//同一个mt，但是在Thread中就不可以，如果用同一
            new Thread(mt).start();//个实例化对象mt，就会出现异常
            new Thread(mt).start();
        }
    };
```

虽然现在程序中有三个线程，但是一共卖了10张票，也就是说使用**Runnable实现多线程可以达到资源共享目的**。



### 一个HADOOP环境，整合了HBASE和HIVE，是否有必要给HDFS和HBASE都分别配置压缩策略？请给出对压缩策略的建议。

hdfs在存储的时候不会将数据进行压缩，如果想进行压缩，我们可以在向 hdfs上传数据的时候进行压缩。

#### 1. 采用压缩流

```java
//压缩文件
public static void compress(String codecClassName) throws Exception{
	Class<?> codecClass = Class.forName(codecClassName);
	Configuration conf = new Configuration();
	FileSystem fs = FileSystem.get(conf);
	CompressionCodec codec = (CompressionCodec)ReflectionUtils.newInstance(codecClass, conf);
	
    //指定压缩文件路径
	FSDataOutputStream outputStream = fs.create(new Path("/user/hadoop/text.gz"));
	
    //指定要被压缩的文件路径
	FSDataInputStream in = fs.open(new Path("/user/hadoop/aa.txt"));

    //创建压缩输出流
	CompressionOutputStream out = codec.createOutputStream(outputStream);
	IOUtils.copyBytes(in, out, conf);
	IOUtils.closeStream(in);
    IOUtils.closeStream(out);
}
```



#### 2.采用序列化文件

```java
public void testSeqWrite() throws Exception {
    
	Configuration conf = new Configuration();// 创建配置信息
	conf.set("fs.default.name", "hdfs://master:9000");// hdfs默认路径
	conf.set("hadoop.job.ugi", "hadoop,hadoop");// 用户和组信息
    
	String uriin = "hdfs://master:9000/ceshi2/";// 文件路径
	FileSystem fs = FileSystem.get(URI.create(uriin), conf);// 创建filesystem
	Path path = new Path("hdfs://master:9000/ceshi3/test.seq");// 文件名
    
	IntWritable k = new IntWritable();// key，相当于 int
	Text v = new Text();// value，相当于 String
	SequenceFile.Writer w = SequenceFile.createWriter(fs, conf, path,k.getClass(), v.getClass());// 创建 writer

    for (int i = 1; i < 100; i++) {// 循环添加
        k.set(i);
        v.set("abcd");
        w.append(k, v);
	}
    
	w.close();
	IOUtils.closeStream(w);// 关闭的时候 flush
	fs.close();
}
```

hbase为列存数据库，本身存在压缩机制，所以无需设计。



### ❤️简述Hbase性能优化的思路

1. 在库表设计的时候，尽量考虑rowkey和columnfamily的特性
2. 进行hbase集群的调优：见hbase调优



### 简述Hbase filter的实现原理是什么？结合实际项目经验，写出几个使用filter的场景

hbase的filter是通过scan设置的，所以是基于scan的查询结果进行过滤

1、 在进行订单开发的时候，我们使用 rowkeyfilter过滤出某个用户的所有订单。
2、 在进行云笔记开发时，我们使用 rowkey过滤器进行 redis数据的恢复。



### ROWKEY的后缀匹配怎么实现？例如ROWKEY是yyyyMMDD-UserID形式，如UserID为条件查询数据，怎么实现

使用rowkey过滤器实现



