---
title: 大数据面试题100道
date: 2021-08-25 12:50:07
permalink: /pages/5ff074/
categories:
  - 更多
  - 面试
tags:
  - 
---

# 大数据工程师面试题

## 选择题

1. 下面哪个程序负责HDFS数据存储。

   a) NameNode 	b) Jobtracker 	c) Datanode 	d) secondaryNameNode 	e) tasktracker 

   答案 C datanode

   

2. HDfS 中的 block 默认保存几份？ 

   a) 3 份 	b) 2 份 	c) 1 份 	d) 不确定 

   答案 A 默认 3 份

   

3. 下列哪个程序通常与 NameNode 在一个节点启动? 

   a) SecondaryNameNode 	b) DataNode 	c) TaskTracker 	d) Jobtracker 

   答案 D

   

4. HDFS 默认 Block Size 

   a) 32MB	b) 64MB	c) 128MB 

   答案：B

   

5. 下列哪项通常是集群的最主要瓶颈 

   a) CPU 	b) 网络 	c) 磁盘 IO 	d) 内存 

   答案：C 磁盘

> 首先集群的目的是为了节省成本，用廉价的 pc 机，取代小型机及大型机。小型机和大型机 有什么特点？ 
>
> 1.cpu 处理能力强 
>
> 2.内存够大，所以集群的瓶颈不可能是 a 和 d 
>
> 3.如果是互联网有瓶颈，可以让集群搭建内网。每次写入数据都要通过网络（集群是内网）， 然后还要写入 3 份数据，所以 IO 就会打折扣。



6. 关于 SecondaryNameNode 哪项是正确的？

   a) 它是 NameNode 的热备 	b) 它对内存没有要求 	c) 它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间 	d) SecondaryNameNode 应与 NameNode 部署到一个节点 

   答案 C

   

7. 下列哪项可以作为集群的管理？ 

   a) Puppet 	b) Pdsh 	c) Cloudera Manager d) Zookeeper 

   答案 ABD 	具体可查看什么是 Zookeeper，Zookeeper 的作用是什么，在 Hadoop 及 hbase 中具体作 用是什么。

   

8. Client 端上传文件的时候下列哪项正确 

   a) 数据经过 NameNode 传递给 DataNode 	b) Client 端将文件切分为 Block，依次上传 	c) Client 只上传数据到一台 DataNode，然后由 NameNode 负责 Block 复制工作 

   答案 B 分析：Client 向 NameNode 发起文件写入的请求。NameNode 根据文件大小和文件块配置 情况，返回给 Client 它所管理部分 DataNode 的信息。Client 将文件划分为多个 Block，根据 DataNode 的地址信息，按顺序写入到每一个 DataNode 块中。具体查看 HDFS 体系结构简介及优缺点。



9. 下列哪个是 Hadoop 运行的模式 

   a) 单机版 	b) 伪分布式 	c) 分布式 

   答案 ABC 单机版,伪分布式只是学习用的。

---

## 面试题

### Hadoop 的核心配置是什么？ 

Hadoop 的核心配置通过两个 xml 文件来完成：1，hadoop-default.xml；2，hadoop-site.xml。 这些文件都使用 xml 格式，因此每个 xml 中都有一些属性，包括名称和值，但是当下这些文 件都已不复存在。



### 那当下又该如何配置？

Hadoop 现在拥有 3 个配置文件：1，core-site.xml；2，hdfs-site.xml；3，mapred-site.xml。这 些文件都保存在 conf/子目录下。



### “jps”命令的用处？

 这个命令可以检查 Namenode、Datanode、Task Tracker、 Job Tracker 是否正常工作。



### ❤️mapreduce 的原理?

![image-20210825130550811](https://gitee.com/vicxsl/img/raw/master/img/1629867951423/image-20210825130550811.png)

1. 单行文件内容作为输入
2. 分布式的DataNode处理数据
   1. 通过Mapper处理文件，处理偏移量和数据分割
   2. content 循环输出分割好的内容
3. 拷贝数据到其他DataNode（分区好之后？）
4. Reducer处理，加工后输出数据

### HDFS 存储的机制?

### HDFS 写流程

![image-20210825132443536](https://gitee.com/vicxsl/img/raw/master/img/1629869083912/image-20210825132443536.png)

流程： 

1.  client 链接 namenode 存数据 

2. namenode 记录一条数据位置信息（元数据），告诉 client 存哪。 

3. client 用 hdfs 的 api 将数据块（默认是 64M）存储到 datanode 上。 

4.  datanode 将数据水平备份。并且备份完将反馈 client。 

5. client 通知 namenode 存储块完毕。 

6. namenode 将元数据同步到内存中。 

7. 另一块循环上面的过程。

   

### HDFS读流程

![image-20210825132702811](https://gitee.com/vicxsl/img/raw/master/img/1629869223178/image-20210825132702811.png)

流程：

1. client 链接 namenode，查看元数据，找到数据的存储位置。 2. 
2. client 通过 hdfs 的 api 并发读取数据。 
3. 关闭连接。



### 举一个简单的例子说明 mapreduce 是怎么来运行的 ?

wordcount 的例子



### 用 mapreduce 来实现下面需求？ 

现在有 10 个文件夹,每个文件夹都有 1000000 个 url.现在让你找出 top1000000url。 

解答：topk



(还可以用 treeMap, 到 1000000 了每来一个都加进去, 删掉最小的)



### hadoop 中 Combiner 的作用?

combiner 是 reduce 的实现，在 map 端运行计算任务，减少 map 端的输出数据。 作用就是优化。 

但是 combiner 的使用场景是 mapreduce 的 map 和 reduce 输入输出一样。



### 简述 hadoop 安装

![image-20210825133000964](https://gitee.com/vicxsl/img/raw/master/img/1629869401349/image-20210825133000964.png)

1. 创建 hadoop 帐户.
2. setup.改IP.
3. 安装java，并修改/etc/profile 文件，配置 java 的环境变量.
4. 修改 Host 文件域名.
5. 安装 SSH，配置无密钥通信.
6. 解压hadoop.
7. 配置 conf 文件下 hadoop-env.sh、core-site.sh、 mapre-site.sh、hdfs-site.sh.
8. 配置hadoop的环境变量,
9. Hadoop namenode -format
10. Start-all



### 请列出 hadoop 进程名

![image-20210825133236037](https://gitee.com/vicxsl/img/raw/master/img/1629869556412/image-20210825133236037.png)

1. namenode : 管理集群，并记录 datanode 文件信息.
2. Secondname:可以做冷备，对一定范围内数据做快照性备份.
3. Datanode:存储数据
4. Jobtracker :管理任务，并将任务分配给 tasktracker.
5. Tasktracker:任务执行方.



### 解决下面的错误

![image-20210825133409119](https://gitee.com/vicxsl/img/raw/master/img/1629869649566/image-20210825133409119.png)

1、 权限问题，可能曾经用 root启动过集群。 ( 例如 hadoop搭建的集群 ,是tmp/hadoop hadoop/..... )
2、 可能是文件夹不存在( Directory does not exist )
3、 解决 : 删掉 tmp下的那个文件 ,或改成当前用户（ Directory /tmp/hadoop-root/ ）



### 写出下面的命令

![image-20210825133650988](https://gitee.com/vicxsl/img/raw/master/img/1629869811385/image-20210825133650988.png)

1. hadoop job -list 

2. 拿到job-id  shadoop job -kill job-id

3. Hadoop fs -rmr /tmp/aaa

4. 加新节点时:
   Hadoop-daemon.sh start datanode

   Hadoop-daemon.sh start tasktracker

5. 删除时:

   Hadoop mradmin -refreshnodes
   Hadoop dfsadmin -refreshnodes



### 简述 hadoop的调度器

![image-20210826131756492](https://gitee.com/vicxsl/img/raw/master/img/1629955077154/image-20210826131756492.png)

Fifo schedular : 默认，先进先出的原则。
Capacity schedular : 计算能力调度器，选择占用最小、优先级高的先执行，依此类推。
Fair schedular: 公平调度，所有的 job 具有相同的资源。



### 列出你开发 mapreduce的语言

![image-20210826131927559](https://gitee.com/vicxsl/img/raw/master/img/1629955167927/image-20210826131927559.png)

java



### 书写程序

![image-20210826132034992](https://gitee.com/vicxsl/img/raw/master/img/1629955235358/image-20210826132034992.png)

wordcount



### 不同语言的优缺点

![image-20210826132057466](https://gitee.com/vicxsl/img/raw/master/img/1629955257822/image-20210826132057466.png)

hadoop

hadoop是 java写的， java的集成效果最好，并且平台环境统一。



### hive有哪些保存元数据的方式，个有什么特点。

![image-20210826132128077](https://gitee.com/vicxsl/img/raw/master/img/1629955288440/image-20210826132128077.png)

1、 内存数据库 derby，安装小，但是数据存在内存，不稳定
2、 mysql数据库，数据存储模式可以自己设置，持久化好，查看方便。



combiner和 partition的作用

![image-20210826132158158](https://gitee.com/vicxsl/img/raw/master/img/1629955318528/image-20210826132158158.png)

combiner是 reduce的实现，在 map端运行计算任务，减少 map端的输出数据。
**作用就是优化。**
但是combiner的使用场景是mapreduce的 map输出结果和 reduce输入输出一样。



**partition的默认实现是 hashpartition，是 map端将数据按照 reduce个数取余**，进行分区，
不同的 reduce来 copy自己的数据。
partition的作用是将数据分到不同的 reduce进行计算，加快计算效果。



### hive内部表和外部表的区别

内部表：加载数据到 hive所在的 hdfs目录， 删除时，元数据和数据文件都删除。
外部表：不加载数据到 hive所在的 hdfs目录，删除时，只删除表结构。



### hbase的 rowkey怎么创建好？列族怎么创建比较好？

hbase存储时，数据按照 Row key的字典序 (byte order)排序存储。设计 key时，要充分排序
存储这个特性，将经常一起读取的行存储放到一起。 (位置相关性 )

一个列族在数据底层是一个文件，所以将经常一起查询的列放到一个列族中，列族尽量少，减少文件的寻址时间。减少文件的寻址时间。



### 用 mapreduce怎么处理数据倾斜问题？

数据倾斜：

map /reduce程序执行时， reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个 key的条数比其他key多很多（有时是百倍或者千倍之多），这条 key所在的 reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完，此称之为数据倾斜。
用hadoop程序进行数据关联时，常碰到数据倾斜的情况，这里提供一种解决方法。

自己实现**partition类，用** **key和 value相加取 hash值**：

方式1：
源代码：

```java
public int getPartition(K key, V value,int numReduceTasks) 
{ 
    return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
}

修改后
public int getPartition(K key, V value,int numReduceTasks) 
{ 
    return ((（key).hashCode()+value.hashCode())& Integer.MAX_VALUE) % numReduceTasks;
}
```



方式2：

```java
public class HashPartitioner<K, V> extends Partitioner<K, V> {
private int aa= 0;
    
/** Use {@link Object#hashCode()} to partition. */
public int getPartition(K key, V value,int numReduceTasks) 
{
return (key.hashCode()+(aa++) & Integer.MAX_VALUE) % numReduceTasks;
}
```

> 重新分区器



### hadoop框架中怎么来优化

1. 从应用程序角度进行优化。由于 mapreduce是迭代逐行解析数据文件的，怎样在迭代的情况下，编写高效率的应用程序，是一种优化思路。
2.  对 Hadoop参数进行调优。当前 hadoop系统有 190多个配置参数，怎样调整这些参数，使 hadoop作业运行尽可能的快，也是一种优化思路。
3. 从系统实现角度进行优化。这种优化难度是最大的，它是从 hadoop实现机制角度，发现当前 Hadoop设计和实现上的缺点，然后进行源码级地修改。该方法虽难度大，但往往效果明显。
4. linux内核参数调整



### 从应用程序角度进行优化

1. 避免不必要的 reduce任务

   如果mapreduce程序中 reduce是不必要的，那么我们可以在 map中处理数据 , Reducer设置
   为 0。这样 避免了多余的 reduce任务。

2. 为 job添加一个 Combiner

   为job添加一个 combiner可以大大减少 shuffle阶段从 map task拷贝给远程 reduce task的数据量。一般而言， combiner与 reducer相同。

3. 根据处理数据特征使用最适合和简洁的 Writable类型
   Text对象使用起来很方便，但它在由数值转换到文本或是由 UTF8字符串转换到文本时都是低效的，且会消耗大量的 CPU时间。当处理那些非文本的数据时，可以使用二进制的 Writable类型，如 IntWritable FloatWritable等。二进制 writable好处：避免文件转换的消耗；使map task中间结果占用更少的空间。

4. 重用 Writable类型
   很多MapReduce用户常犯的一个错误是，在一个 map/reduce方法中为每个输出都创建Writable对象。例如，你的 Wordcout mapper方法可能这样写：

   ```java
   public void map(...) {
   …
   for (String word : words) {
   	output.collect(new Text(word), new IntWritable(1));
   	}
   }
   ```

   这样会导致程序分配出成千上万个短周期的对象。Java垃圾收集器就要为此做很多的工作。垃圾收集器就要为此做很多的工作。更有效的写法是：更有效的写法是：

   抽取对象

   ```java
   class MyMapper … {
   Text wordText = new Text();
       
   IntWritable one = new IntWritable(1);
       
   public void map(...) {
       for (String word: words) {
               wordText.set(word);
               output.collect(wordText, one);
           }
   	}
   }
   ```

   

5.  使用使用StringBuffer而不是而不是String

   当需要对字符串进行操作时，**使用StringBuffer而不是而不是String，String是是read--only的，如果对的，如果对它进行修改，会产生临时对象**，而StringBuffer是可修改的，不会产生临时对象。是可修改的，不会产生临时对象。
   

### 对参数进行调优

查看linux的服务，可以关闭不必要的服务

 ntsysv 



**停止打印服务**

 #/etc/init.d/cups stop

chkconfig cups off



**关闭ipv6** 

vim /etc/modprobe.conf 

添加内容 

alias net-pf-10 off alias ipv6 off



**调整文件最大打开数** 

查看： ulimit -a  结果：open files (-n) 1024 

临时修改： ulimit -n 4096 

持久修改：

 vi /etc/security/limits.conf在文件最后加上：

* soft nofile 65535
* hard nofile 65535 
* soft nproc 65535 
* hard nproc 65535



**修改linux内核参数**

vi /etc/sysctl.conf 

添加 

net.core.somaxconn = 32768 

web应用中listen函数的backlog默认会给我们内核参数的net.core.somaxconn限制到128，而nginx定义的NGX_LISTEN_BACKLOG默认为511，所以有必要调整这个值。 

调整swap分区什么时候使用：

查看：cat /proc/sys/vm/swappiness 

设置：vi /etc/sysctl.conf 

​	在这个文档的最后加上这样一行: vm.swappiness=10 

​	表示物理内存使用到90%（100-10=90）的时候才使用swap交换区



**关闭noatime**

vi /etc/fstab /dev/sda2/data ext3	 noatime,nodiratime 0 0



**设置readahead buffer** 

blockdev --setra READAHEAD 512 /dev/sda
一下是修改mapred-site.xml文件



**修改最大槽位数** 

槽位数是在各个tasktracker上的mapred-site.xml上设置的，默认都是2

```properties
<property> 
<name>mapred.tasktracker.map.tasks.maximum</name> 
#++++maptask的最大数
<value>2</value>
</property> 

<property> 
<name>mapred.tasktracker.reduce.tasks.maximum</name> 
#++++reducetask的最大数 
<value>2</value> 
</property>
```



**调整心跳间隔**

集群规模小于300时，心跳间隔为300毫秒 

```properties
mapreduce.jobtracker.heartbeat.interval.min 心跳时间 

mapred.heartbeats.in.second 集群每增加多少节点，时间增加下面的值 

mapreduce.jobtracker.heartbeat.scaling.factor 集群每增加上面的个数，心跳增多少
```



**启动带外心跳** 

mapreduce.tasktracker.outofband.heartbeat 默认是false



配置多块磁盘 

mapreduce.local.dir 

配置RPC hander数目 

mapred.job.tracker.handler.count 默认是10，可以改成50，根据机器的能力